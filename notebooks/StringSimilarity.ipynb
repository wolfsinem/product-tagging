{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('/Users/wolfsinem/product-tagging/')\n",
    "from extended_df import model_dataframe\n",
    "\n",
    "sys.path.append('/Users/wolfsinem/product-tagging/product_tagging')\n",
    "from tags_generator import tokenize_string\n",
    "\n",
    "#python built in library to calculate the similarity\n",
    "from difflib import SequenceMatcher\n",
    "import difflib\n",
    "\n",
    "import nltk \n",
    "# nltk.download('averaged_perceptron_tagger') # download once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = model_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[19967]['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar(string1, string2):\n",
    "    \"\"\"Function to calculate the similarity of two (product) descriptions.\n",
    "    \n",
    "    :param string1: This would be the first textual description. \n",
    "    :type string1: string\n",
    "    \n",
    "    :param string2: This would be the second textual description.\n",
    "    :type string2: string\n",
    "    \"\"\"\n",
    "    sequence = SequenceMatcher(None, string1, string2)\n",
    "    score = sequence.ratio()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As you can see, even though I dropped all of the duplicates, product descriptions with a similarity less than 100% stay. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar(df.loc[19970]['description'],df.loc[19967]['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description a and b are exactly the same, thus a similarity score of 1, thus 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0]['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[3]['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['Buy Wallmantra Extra Large Vinyl Stickers Sticker for Rs.2194 online. Wallmantra Extra Large Vinyl Stickers Sticker at best prices with FREE shipping & cash on delivery. Only Genuine Products. 30 Day Replacement Guarantee.']\n",
    "b = ['Buy Wallmantra Extra Large Vinyl Stickers Sticker for Rs.2194 online. Wallmantra Extra Large Vinyl Stickers Sticker at best prices with FREE shipping & cash on delivery. Only Genuine Products. 30 Day Replacement Guarantee.']\n",
    "\n",
    "similar(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "similar(df.loc[0]['description'],df.loc[3]['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_description = df.loc[5]['description']\n",
    "product_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags created by the product tagging algorithm\n",
    "tags = tokenize_string(product_description)\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joined_string = ' '.join(map(str, tags))\n",
    "# joined_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagger = nltk.pos_tag(tags)\n",
    "pos_tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset('NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset('JJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine words like weight - weights so we dont use the 'same' tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_tags = []\n",
    "for i in tags:\n",
    "    stemmed_tags.append(porter.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stemmed_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_tag(set_tags,tags_size): \n",
    "    \"\"\"This function uses the NLTK lemmatizer function in the first part. Lemmatization, unlike Stemming, \n",
    "    reduces the inflected words properly ensuring that the root word belongs to the language\n",
    "    See: https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\n",
    "\n",
    "    To reduce the amount of duplicates in a set of tags we will thus use lemmatization.\n",
    "    Words like 'weight' and 'weights' will be considered the same and be saved\n",
    "    as 'weight'. In addition to that we have a few other conditions to clean the set of tags.\n",
    "    \"\"\"\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    lemm_set = []\n",
    "    for word in tokenize_string(set_tags):\n",
    "        tag = lemmatizer.lemmatize(word)\n",
    "        lemm_set.append(tag)\n",
    "    \n",
    "    lemm_set = list(set(lemm_set))\n",
    "    lemm_set = [x for x in lemm_set if not any(c.isdigit() for c in x)]\n",
    "    lemm_set = [x for x in lemm_set if not x[-3:] == \"ing\"]\n",
    "    \n",
    "            \n",
    "    return [i for i in lemm_set if len(i) > 1][:tags_size] # remove words with single character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lst = lemma_tag(df.loc[300]['description'],3)\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_df(df, tags_size):\n",
    "    \"\"\"This function extends the original dataframe with an extra column 'tags'.\n",
    "    This function uses both the lemma_tag() and tokenize_user_text_input()\n",
    "    function to tokenize and clean the set of tags.\n",
    "\n",
    "    :param df: This would be the orginal df imported by the user.\n",
    "    :type df: string.\n",
    "    \"\"\"\n",
    "\n",
    "    for i in df.index:\n",
    "        df.at[i,'tags'] = lemma_tag(df.loc[i]['description'], tags_size)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extend_df(df,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imagine we tag every product using the new updated lemm tagger; let's see what the occurence is of each tag we have given to a product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag every product description and store it in a new array\n",
    "df = model_dataframe()\n",
    "tags = []\n",
    "for i in df.index:\n",
    "    tags.append(lemma_tag(df.loc[i]['description']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the occurenece with the Counter function\n",
    "word_counts = Counter(word for words in tags for word in words)\n",
    "word_counts.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('There are {} unique tags'.format(len(word_counts.most_common())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts.most_common() # see if we can delete words like buy free flipkart etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the lemmatizer function we can track down the duplicates in a set of tags and delete them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar('weight','weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar('walking','walk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar('reading','read')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar('hoping','hope')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar('helping','help')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar('stressed','stress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diff_set = []\n",
    "for i in tags:\n",
    "    diff_set.append(difflib.get_close_matches(i, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "diff_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_set[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_set[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_rate(description):\n",
    "    \"\"\"This function calculates the similarity score between words that are similar to eachother.\n",
    "    \n",
    "    :param description: A product description. \n",
    "    :type description: string\n",
    "    \"\"\"\n",
    "    \n",
    "    tagged_list = tokenize_string(description)\n",
    "    \n",
    "    diff_set = []\n",
    "    for i in tagged_list:\n",
    "        diff_set.append(difflib.get_close_matches(i, tagged_list))\n",
    "    \n",
    "    scores = []\n",
    "    for i in range(len(diff_set)):\n",
    "        if not len(diff_set[i]) <= 1:\n",
    "            firstW = diff_set[i][0]\n",
    "            secondW = diff_set[i][1]\n",
    "            similarityScore = similar(diff_set[i][0],diff_set[i][1])\n",
    "            scores.append([firstW, secondW, similarityScore])\n",
    "#             print(\"Score of similarity for {} and {} is: {}\".format(firstW, secondW, similarityScore))\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_diff = similarity_rate(df.loc[5]['description'])\n",
    "scores_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python38064bit94409e1e4df94da1b5cc700cc0e6ab29"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}