{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Language processing\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk\n",
    "# nltk.download('stopwords') # if you haven't downloaded this yet.\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "sys.path.append('/Users/wolfsinem/product-tagging/static/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_csv(path=\"/Users/wolfsinem/product-tagging/static/data/uploads\"):\n",
    "    \"\"\"returns the latest uploaded file in the given folder path with the conditions\n",
    "    of it being a csv file and starting with csv.\n",
    "    \n",
    "    :param path: path of the directory in which the uploads are saved.\n",
    "    :type path: string.\n",
    "    \"\"\"\n",
    "    \n",
    "    files = os.listdir(path)\n",
    "    for fname in files:\n",
    "        if fname.startswith('sub') and fname.endswith(\".csv\"):\n",
    "            paths = [os.path.join(path, basename) for basename in files]\n",
    "            return max(paths, key=os.path.getctime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# of course every dataset is saved differenly. Therefore its hard to figure out how we can exactly filter out the two columns\n",
    "# product_name and description of theyre saved differenly in the dataset. Especially when we havent seen the dataset before\n",
    "# and its randomy uploaded by a user. A solution for this is to ask the person uploading the dataset to make sure\n",
    "# the columns are saved correctly as 'product_name' and 'description'\n",
    "print(unedited.filter(like='product_name').columns)\n",
    "print(unedited.filter(like='description').columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_dataframe():\n",
    "    \"\"\"This function will check if the dataset which is uploaded by the user has both columns \n",
    "    product_name and description. If yes, make a new model dataframe with an \n",
    "    empty tags column so we can later add new tags to it.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(retrieve_csv())\n",
    "    \n",
    "    df.dropna(inplace=True)\n",
    "    df.drop_duplicates(subset=['description'],inplace=True)\n",
    "    \n",
    "    if 'product_name' in df.columns and 'description' in df.columns: \n",
    "        model_df = df[['product_name','description']]\n",
    "        pd.options.mode.chained_assignment = None \n",
    "        model_df['tags'] = \"\"\n",
    "    else:\n",
    "        raise ValueError(\"Columns product_name and description don't exist, please rename the column names\")\n",
    "    return model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_user_text_input(sentence, size_tags=10):\n",
    "    \"\"\"This function splits a string into substrings using a regular expression\n",
    "     using RegexpTokenizer. Additionally it counts the occurence of each word\n",
    "     and returns the top x words which can be used as tags\n",
    "\n",
    "    :param sentence: Text description of a product\n",
    "    :type sentence: string\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    new_words = tokenizer.tokenize(str(sentence))\n",
    "    new_words = [token.lower() for token in new_words]\n",
    "    \n",
    "    stop_words = set(stopwords.words('english')) \n",
    "\n",
    "    filter_tokens = [w for w in new_words if not w in stop_words]\n",
    "    count_terms = Counter(filter_tokens).most_common(size_tags)\n",
    "    count_terms = [item[0] for item in count_terms]\n",
    "\n",
    "    token_lists = []\n",
    "    for i in count_terms:\n",
    "        token_lists.append(i)\n",
    "    \n",
    "    token_lists = [item for item in token_lists if not item.isdigit()]\n",
    "    \n",
    "    return token_lists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_tag(sentence): \n",
    "    \"\"\"This function uses the NLTK lemmatizer function in the first part. Lemmatization, unlike Stemming, \n",
    "    reduces the inflected words properly ensuring that the root word belongs to the language\n",
    "    See: https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\n",
    "\n",
    "    To reduce the amount of duplicates in a set of tags we will thus use lemmatization.\n",
    "    Words like 'weight' and 'weights' will be considered the same and be saved\n",
    "    as 'weight'. In addition to that we have a few other conditions to clean the set of tags.\n",
    "    \"\"\"\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    lemm_set = []\n",
    "    for word in tokenize_user_text_input(sentence):\n",
    "        tag = lemmatizer.lemmatize(word)\n",
    "        lemm_set.append(tag)\n",
    "    \n",
    "    lemm_set = list(set(lemm_set))\n",
    "    lemm_set = [x for x in lemm_set if not x[-3:] == \"ing\"]\n",
    "    \n",
    "    return [i for i in lemm_set if len(i) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_df(df=model_dataframe()):\n",
    "    \"\"\"This function extends the original dataframe with an extra column 'tags'.\n",
    "    This function uses both the lemma_tag() and tokenize_user_text_input() function to tokenize \n",
    "    and clean the set of tags.\n",
    "    \n",
    "    :param df: This would be the orginal df imported by the user.\n",
    "    :type df: string.\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in df.index:\n",
    "        df.at[i,'tags'] = lemma_tag(df.loc[i]['description'])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_extendedDF(path=\"/Users/wolfsinem/product-tagging/static/data/exports\", df=model_dataframe()):\n",
    "    \"\"\"This function exports the created extended subset into the right folder.\n",
    "    From this folder the file can be exported to the right user again.\n",
    "\n",
    "    :param path: This is the path which the file is exported to.\n",
    "    :type path: string.\n",
    "\n",
    "    :param df: This would be the orginal df imported by the user.\n",
    "    :type df: string.\n",
    "    \"\"\"\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        outname = \"extended-\" + os.path.basename(os.path.normpath(retrieve_csv()))\n",
    "        full_path = os.path.join(path, outname) \n",
    "        df.to_csv(full_path)\n",
    "    else:\n",
    "        raise ValueError(\"This {} path file does not exist\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_extendedDF()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python38064bit94409e1e4df94da1b5cc700cc0e6ab29"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}